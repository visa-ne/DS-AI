{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NLTK package in python \n",
    "Contents:\n",
    "    1. Tokenization \n",
    "    2. Ngrams \n",
    "    3. Pos_tagging \n",
    "    4. TF-IDF (Feature Extraction) \n",
    "    5. NER \n",
    "    6. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Machine learning is the science of getting computers to act without being explicitly programmed. \n",
    "In the past decade, machine learning has given us self-driving cars, \n",
    "practical speech recognition, effective web search, \n",
    "and a vastly improved understanding of the human genome. \n",
    "Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it.\n",
    "Many researchers also think it is the best way to make progress towards human-level AI. \n",
    "In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself.\n",
    "More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems.\n",
    "Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Data Science is a new field and applied to many different domains.\"\n",
    "sent_tokens = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Science is a new field and applied to many different domains.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = ngrams(token,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Data', 'Science')\n",
      "('Science', 'is')\n",
      "('is', 'a')\n",
      "('a', 'new')\n",
      "('new', 'field')\n",
      "('field', 'and')\n",
      "('and', 'applied')\n",
      "('applied', 'to')\n",
      "('to', 'many')\n",
      "('many', 'different')\n",
      "('different', 'domains')\n",
      "('domains', '.')\n"
     ]
    }
   ],
   "source": [
    "for k in bigrams:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = ngrams(token,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Data', 'Science', 'is')\n",
      "('Science', 'is', 'a')\n",
      "('is', 'a', 'new')\n",
      "('a', 'new', 'field')\n",
      "('new', 'field', 'and')\n",
      "('field', 'and', 'applied')\n",
      "('and', 'applied', 'to')\n",
      "('applied', 'to', 'many')\n",
      "('to', 'many', 'different')\n",
      "('many', 'different', 'domains')\n",
      "('different', 'domains', '.')\n"
     ]
    }
   ],
   "source": [
    "for k in trigrams:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grams in bigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a machine learning course, we teach data science.\n",
      "We use R and Python\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_token:\n",
    "    print sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Science is a new field and applied to many different domains.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This is the second second document.',\n",
    "...     'And the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_df and min_df can take values as proportion or count \n",
    "vectorizer = TfidfVectorizer(min_df = 2, max_df=4, ngram_range=(1,2), use_idf=False) \n",
    "a = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n",
       "        0.33333333, 0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.40824829, 0.        , 0.        , 0.40824829, 0.40824829,\n",
       "        0.40824829, 0.        , 0.40824829, 0.40824829],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.37796447, 0.37796447, 0.37796447, 0.37796447, 0.        ,\n",
       "        0.37796447, 0.37796447, 0.37796447, 0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cntvect = CountVectorizer()\n",
    "b = cntvect.fit_transform(corpus)\n",
    "b.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "postags = pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'NNP'),\n",
       " ('Science', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('field', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('applied', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('many', 'JJ'),\n",
       " ('different', 'JJ'),\n",
       " ('domains', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('VBG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "1. Using NER and NERT tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple India Pvt Ltd', 'NER', 'Hyderabad', 'India', 'Microsoft Corporation', 'CEO Satya Nadella'}\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "with open('sample.txt', 'r') as f:\n",
    "    sample = f.read() # f.readlines() -- to read each line as a separate row \n",
    "\n",
    "\n",
    "sentences = nltk.sent_tokenize(sample)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "entity_names = []\n",
    "for tree in chunked_sentences:\n",
    "    # Print results per sentence\n",
    "    # print extract_entity_names(tree)\n",
    "\n",
    "    entity_names.extend(extract_entity_names(tree))\n",
    "\n",
    "# Print all entity names\n",
    "#print entity_names\n",
    "\n",
    "# Print unique entity names\n",
    "print(set(entity_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming and lemmatization \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \" This is a test of how Stemmer stems or lemmatizes. Programmer builds programs. Programming is a skill. Execution of different programs are tasks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stems',\n",
       " 'or',\n",
       " 'lemmatizes',\n",
       " '.',\n",
       " 'Programmer',\n",
       " 'builds',\n",
       " 'programs',\n",
       " '.',\n",
       " 'Programming',\n",
       " 'is',\n",
       " 'a',\n",
       " 'skill',\n",
       " '.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[7:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', 'of', 'how', 'Stemmer', 'stem', 'or', 'lemmatizes', '.', 'Programmer', 'build', 'program', '.', 'Programming', 'is', 'a', 'skill', '.', 'Execution', 'of', 'different', 'program', 'are', 'task']\n"
     ]
    }
   ],
   "source": [
    "## Lemmatization \n",
    "lemm = WordNetLemmatizer()\n",
    "lem_words = [ lemm.lemmatize(w) for w in words] ## List comprehension\n",
    "print(lem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'is', 'a', 'test', 'of', 'how', 'stemmer', 'stem', 'or', 'lemmat', '.', 'programm', 'build', 'program', '.', 'program', 'is', 'a', 'skill', '.', 'execut', 'of', 'differ', 'program', 'are', 'task']\n"
     ]
    }
   ],
   "source": [
    "## Using Porter stemmer \n",
    "ps = PorterStemmer()\n",
    "ps_words = [ps.stem(w) for w in words]\n",
    "print(ps_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'is', 'a', 'test', 'of', 'how', 'stem', 'stem', 'or', 'lem', '.', 'program', 'build', 'program', '.', 'program', 'is', 'a', 'skil', '.', 'execut', 'of', 'diff', 'program', 'ar', 'task']\n"
     ]
    }
   ],
   "source": [
    "# Using Lancaster Stemmer \n",
    "ls = LancasterStemmer()\n",
    "ls_words = [ls.stem(w) for w in words]\n",
    "print(ls_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'stemmer', 'stems', 'lemmatizes', '.', 'programmer', 'builds', 'programs', '.', 'programming', 'skill', '.', 'execution', 'different', 'programs', 'tasks']\n"
     ]
    }
   ],
   "source": [
    "## Remove stop words \n",
    "stop_words = stopwords.words('english')\n",
    "mywords = ['test','this']\n",
    "stop_words = stop_words+mywords\n",
    "clean_words = [w for w in words if not w in stop_words]\n",
    "clean_words = [ w.lower() for w in clean_words]\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words[0:10])\n",
    "mywords = [\"test\",\"this\"]\n",
    "stop_words.extend(mywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
